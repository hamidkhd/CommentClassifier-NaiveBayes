{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Artificial Intelligence \n",
    "                                                                                                                   \n",
    "### Third project report  \n",
    "\n",
    "Hamidreza Khodadadi 810197499        \n",
    "\n",
    "#### The project goal\n",
    "In this project, we intend to use the naive bayes classifier algorithm to analyze the attitude of the comments and identify the positive and negative comments according to their text.\n",
    "                                                                                                                 \n",
    "#### Summary:\n",
    "To process and evaluate the comments we have, we act as follows:\n",
    "First, to make processing easier, we consider the title and text of the comment as a type of data.\n",
    "Then, based on the training data we have, for each word in this data, we calculate the two probabilities of being recommended and not being recommended.\n",
    "To do this, we pre-process the data before doing anything. To do this, we remove punctuation and special characters from the text. We defined these signs at the beginning of the project. Then we remove the stop Words in the text. To do this, we use the Stop Words dictionary in the hazm library. Then we normalize the remaining text to adjust the spaces and half-spaces.  \n",
    "Then, using the hazm library functions, we break down the text into words based on the distance between them.\n",
    "Then we can use stemming and lemmatization methods to process words.\n",
    "Of course, each of the above operations may have an adverse effect on the final performance of the program, and we must use them according to our needs.\n",
    "Once the preprocessing operation is complete, we can use the additive smoothing operation for when one word of the test data is not in the training data or when one of the words in the recommended word list is not in the recommended word list, or vice versa. Let's use.\n",
    "This algorithm is described below.\n",
    "In the following, we use Bayesian probability theorem to evaluate texts. Thus, considering the total data and the total recommended data and the total non-recommended data, we calculate and consider two general probabilities of being recommended or not recommended for all words.\n",
    "We then process the entire test file data, multiplying these two high probability values separately by the probability of being recommended or not recommended for each word in the text, the final value of each of the two probabilities that Increased, we categorize that text according to that probability.\n",
    "Instead of multiplying the probabilities, we used the sum of their logarithms to make the calculations more accurate.\n",
    "                                                                                                                      \n",
    "#### Keyword:\n",
    "pre processing - training data - test data - Bayesian law - hazm - additive smoothing - stemming - lemmatization \n",
    "\n",
    "#### First phase \n",
    "In this phase, we perform preprocessing operations on both training and test data.  At this stage, we consider the two data of title and comment as one.  And then we delete its special characters.  Then we delete the stop Words in the data.  And then we normalize both data.  Then we do two operations smoothing and lemmatization on this data.  Performing each operation has some effect on the error rate.\n",
    "\n",
    "##### Question 1:\n",
    "In stemming we remove word for word from the end of the word to reach a meaningful root, but the root we have reached may not be the right root and we may reach a wrong conclusion.\n",
    "In lemmatization, as in the previous step, we look for the root of that word, here we have a source of words that we try to find the correct root for that word according to the words we have and the analysis of the related word.\n",
    "\n",
    "#### Question 2: \n",
    "prior definition: \n",
    "We consider the probability that for the proposed comment mode is equal to the total number of suggested comments on the total number of messages and for the non-suggested mode is equal to the total number of un Suggested comments on the total number of messages Becomes. In this case, we evaluate that comment without having accurate evidence. \n",
    "\n",
    "Posterior definition: \n",
    "In this case, we evaluate each word of that comment and, according to Bayesian law of probability, decide that the comment belongs to the recommended category. Or is in the non-recommended category.\n",
    "\n",
    "Definition of likelihood: \n",
    "In this probability the Bayesian theorem is used and we for each comment for each word in the comment and according to the recommended or not recommended that word and the probability specific to them, the probability of being recommended or recommended We multiply the absence of those words in that comment.\n",
    "\n",
    "Definition of evidence:\n",
    "That a word is in a comment is a proof. This probability is calculated by dividing the number of repetitions of that word in the total data by the total number of words.\n",
    "\n",
    "#### Second phase\n",
    "In this section, we count the number of repetitions of each word for being recommended or not recommended. And given the total number of recommended list words and the total non-recommended list words, we consider two probabilities for each word. One is the probability that the word will be in the recommended category and the other is the probability that the word will be in the non-recommended category.\n",
    "Then, by examining each word in the comments in the test data, and considering the possibility of classifying that word in the recommended or not recommended category, we evaluate by Bayesian law whether that comment is positive or negative.\n",
    "\n",
    "##### Question 3: \n",
    "We estimate the probability of a word being recommended or not based on the number of repetitions of that word in the training data.  But there may be a word in the test data that is not in the recommended or not recommended words we have, or there may be a word in the recommended comment data that is not in the recommended word data, or  Conversely, in this case, when calculating the Bayesian probability for the problem, the probability of the existence of that particular word in the category that does not exist is considered zero.  And he definitely assigns that word to his opponent, but that might be wrong.\n",
    "\n",
    "##### Question 4:\n",
    "We encounter a problem in the third question. We behave in such a way that if there is not a word of the test data words in the list of recommended or non-recommended words obtained based on the training data, it is repeated with one number to that list. We add the words, and if there are no words from the list of recommended words in the list of not recommended words, or vice versa, we add that word to the list with a number of repetitions. Finally, we increase the number of repetitions of the rest of the words in both lists by one so that adding words that did not already exist will not affect the effectiveness of the rest of the data.\n",
    "\n",
    "#### Third phase\n",
    "\n",
    "##### Question 5:\n",
    "Precision is not a good criterion for evaluation. Suppose we have 50 data. This criterion considers 5 of them recommended. And all 5 of them have been correctly identified. The value of this criterion will be equal to one, if among the rest of the data, there were still recommended comments that this criterion could not recognize correctly.\n",
    "We also use the above example for the recall criterion. Suppose half of that data is recommended and the other half is not recommended. And this criterion considers all data recommended. Then the value of this criterion will be equal to one if half of its evaluations are incorrect.\n",
    "\n",
    "##### Question 6:\n",
    "In this type of averaging, we use harmonic averaging. In this type of averaging, we assign the same weight to both recall and precision data. In this type of averaging, when one of our two data has its extreme value, it helps us to analyze more accurately than the other averages.\n",
    "The averaging type is used to average rates.\n",
    "\n",
    "##### Question 7:\n",
    "At the end of the project, the requested outputs are printed.\n",
    "\n",
    "#### Question 8:\n",
    "Initially, the program output is for when we have not performed any operations on the data. Next we have used preprocessing on the data. For preprocessing, we used the operations of deleting punctuation and special characters, deleting Stop Words, normalizing, stemming and lemmatization. Because we used the Stop Words dictionary belonging to the hazm library, some of the desired output We have distanced ourselves. Because this dictionary also includes words that do not need to be deleted.\n",
    "When we do not do the three operations of stemming, lemmazitation and normalization, we get a better result.\n",
    "The stemming operation has a worse performance than the lemmazitiation operation and reduces the performance of the program. But in general, we have achieved a better result than the first case in which we did not perform any operation, and this shows that the operation The preprocessing was generally successful. We used the best possible performance when we used the additive smoothing algorithm. Because we solved the problem that was raised in question 3 and caused a text to be categorized incorrectly in this section. Finally, when we used the two algorithms of preprocessing and additive smoothing, there is some more error than It happened in the previous case because of errors in the functions of the preprocessing algorithm.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import hazm\n",
    "import csv \n",
    "import math\n",
    "\n",
    "special_characters = ['!' '.', '?', ',', '(', ')', '[', ']', '{', '}', '-', '_', '+', '=', '<','>', '*', '&',\n",
    "                      '^', '%', '$', '#', '@', '~', '\"', ':', '/', '|', '؟', '،']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def additive_smoothing(recommended, not_recommended):\n",
    "    for i in recommended:\n",
    "        recommended[i] += 1\n",
    "        if i not in not_recommended:\n",
    "            not_recommended[i] = 1\n",
    "\n",
    "    for x in not_recommended:\n",
    "        not_recommended[x] += 1\n",
    "        if x not in recommended:\n",
    "            recommended[x] = 1\n",
    "\n",
    "    return recommended, not_recommended\n",
    "\n",
    "def stemming(data):\n",
    "    stemmer = hazm.Stemmer()\n",
    "    for i in range(1, len(data)):\n",
    "        word_list = hazm.word_tokenize(data[i][0])\n",
    "\n",
    "        for j in range(len(word_list)):\n",
    "            word_list[j] = stemmer.stem(word_list[j])\n",
    "\n",
    "        _str = str()\n",
    "        for x in word_list:\n",
    "            temp = x + \" \"\n",
    "            _str += temp\n",
    "\n",
    "        data[i][0] = _str\n",
    "    return data\n",
    "\n",
    "def lemmatization(data):\n",
    "    lemmatizer = hazm.Lemmatizer()\n",
    "    for i in range(1, len(data)):\n",
    "        word_list = hazm.word_tokenize(data[i][0])\n",
    "\n",
    "        for j in range(len(word_list)):\n",
    "            word_list[j] = lemmatizer.lemmatize(word_list[j])\n",
    "\n",
    "        _str = str()\n",
    "        for x in word_list:\n",
    "            temp = x + \" \"\n",
    "            _str += temp\n",
    "\n",
    "        data[i][0] = _str\n",
    "    return data\n",
    "\n",
    "def normalize(data):\n",
    "    normalizer = hazm.Normalizer()\n",
    "    for i in range(1, len(data)):\n",
    "        data[i][0] = normalizer.normalize(data[i][0])\n",
    "    return data\n",
    "\n",
    "def remove_stop_words(data):     \n",
    "    for i in range(1, len(data)):\n",
    "        word_list = hazm.word_tokenize(data[i][0])\n",
    "\n",
    "        for j in hazm.stopwords_list():\n",
    "            while j in word_list: word_list.remove(j)  \n",
    "\n",
    "        _str = str()\n",
    "        for x in word_list:\n",
    "            temp = x + \" \"\n",
    "            _str += temp\n",
    "\n",
    "        data[i][0] = _str\n",
    "    return data\n",
    "\n",
    "def remove_special_characters(data):\n",
    "    for i in range(1, len(data)):\n",
    "        word_list = hazm.word_tokenize(data[i][0])\n",
    "\n",
    "        for j in special_characters:\n",
    "            while j in word_list: word_list.remove(j)  \n",
    "\n",
    "        _str = str()\n",
    "        for x in word_list:\n",
    "            temp = x + \" \"\n",
    "            _str += temp\n",
    "\n",
    "        data[i][0] = _str\n",
    "    return data\n",
    "\n",
    "def join_title_and_comment(data):\n",
    "    for i in range(len(data)):\n",
    "       temp = data[i][0] + ' ' + data[i][1]\n",
    "       data[i][0] = temp\n",
    "       data[i].remove(data[i][1])\n",
    "\n",
    "    return data\n",
    "\n",
    "def pre_process(data):\n",
    "    new_data = join_title_and_comment(data)\n",
    "    new_data = remove_special_characters(new_data)\n",
    "    new_data = remove_stop_words(new_data)\n",
    "    new_data = normalize(new_data)\n",
    "    new_data = stemming(new_data)\n",
    "    new_data = lemmatization(new_data)\n",
    "    return new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probability_calculation(train_data, test_data, additive_smoothing_flag):\n",
    "    total_count, recommended_count, not_recommended_count, t_count, r_count, n_count = frequency_calculater(train_data)\n",
    "\n",
    "    if additive_smoothing_flag == True:\n",
    "        recommended_count, not_recommended_count = additive_smoothing(recommended_count, not_recommended_count)\n",
    "\n",
    "    r_dict_sum = dict_sum(recommended_count)\n",
    "    n_r_dict_sum = dict_sum(not_recommended_count)\n",
    "\n",
    "    for i in range(1, len(test_data)):\n",
    "        word_list = hazm.word_tokenize(test_data[i][0])\n",
    "\n",
    "        p_r = math.log10(r_count / t_count)\n",
    "        p_n_r = math.log10(n_count / t_count)\n",
    "\n",
    "        for j in range(len(word_list)):\n",
    "            if word_list[j] not in recommended_count and word_list[j] not in not_recommended_count:\n",
    "                recommended_count[word_list[j]] = 1\n",
    "                not_recommended_count[word_list[j]] = 1\n",
    "                continue\n",
    "\n",
    "            elif word_list[j] not in recommended_count:\n",
    "                recommended_count[word_list[j]] = 1\n",
    "                p_r = float('-inf')\n",
    "                p_n_r += math.log10(not_recommended_count[word_list[j]] / n_r_dict_sum)\n",
    "\n",
    "            elif word_list[j] not in not_recommended_count:\n",
    "                not_recommended_count[word_list[j]] = 1\n",
    "                p_r += math.log10(recommended_count[word_list[j]] / r_dict_sum)\n",
    "                p_n_r = float('-inf')\n",
    "\n",
    "            else:\n",
    "                p_r += math.log10(recommended_count[word_list[j]] / r_dict_sum)\n",
    "                p_n_r += math.log10(not_recommended_count[word_list[j]] / n_r_dict_sum)\n",
    "\n",
    "        if p_n_r < p_r:\n",
    "            test_data[i].append(\"recommended\")\n",
    "        else:\n",
    "            test_data[i].append(\"not_recommended\")\n",
    "        \n",
    "    return test_data\n",
    "\n",
    "def frequency_calculater(data):\n",
    "    total = list()\n",
    "    recommended = list()\n",
    "    not_recommended = list()\n",
    "\n",
    "    t_count = int(0)\n",
    "    r_count = int(0)\n",
    "    n_count = int(0)\n",
    "\n",
    "    for i in range(1, len(data)):\n",
    "        word_list = hazm.word_tokenize(data[i][0])\n",
    "\n",
    "        for j in range(len(word_list)):\n",
    "            total.append(word_list[j])\n",
    "            t_count += 1\n",
    "\n",
    "            if data[i][1] == \"recommended\":\n",
    "                recommended.append(word_list[j])\n",
    "                r_count += 1\n",
    "            else:\n",
    "                not_recommended.append(word_list[j])\n",
    "                n_count += 1\n",
    "\n",
    "    total_count = dict(collections.Counter(total))\n",
    "    recommended_count = dict(collections.Counter(recommended))\n",
    "    not_recommended_count = dict(collections.Counter(not_recommended))\n",
    "\n",
    "    return total_count, recommended_count, not_recommended_count, t_count, r_count, n_count\n",
    "\n",
    "def help_frequency_calculater(data):\n",
    "    return dict(collections.Counter(data))\n",
    "\n",
    "def dict_sum(my_dict):      \n",
    "    _sum = int(0)\n",
    "    for i in my_dict: \n",
    "        _sum += my_dict[i] \n",
    "    return _sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(test_data):\n",
    "    correct = int(0)\n",
    "    total = int(0)\n",
    "\n",
    "    counter = int(0)\n",
    "    wrong = list()\n",
    "\n",
    "    for i in range(1, len(test_data)):\n",
    "        total += 1\n",
    "        if test_data[i][1] == test_data[i][2]:\n",
    "            correct += 1\n",
    "\n",
    "        else:\n",
    "            if counter < 5:\n",
    "                temp = str(i) + \" \" + test_data[i][0] + \" \" + test_data[i][1] + \" \" + test_data[i][2]\n",
    "                wrong.append(temp)\n",
    "                counter += 1\n",
    "\n",
    "    print(\"Accuracy: \", correct / total)\n",
    "\n",
    "    return wrong\n",
    "\n",
    "def precision(test_data):\n",
    "    correct = int(0)\n",
    "    total = int(0)\n",
    "\n",
    "    for i in range(1, len(test_data)):\n",
    "        if test_data[i][2] == \"recommended\":\n",
    "            total += 1\n",
    "            if test_data[i][1] == test_data[i][2]:\n",
    "                correct += 1\n",
    "    print(\"Precision: \", correct / total)\n",
    "    return correct / total\n",
    "\n",
    "def recall(test_data):\n",
    "    correct = int(0)\n",
    "    total = int(0)\n",
    "\n",
    "    for i in range(1, len(test_data)):\n",
    "        if test_data[i][1] == \"recommended\":\n",
    "            total += 1\n",
    "\n",
    "        if test_data[i][1] == test_data[i][2] and test_data[i][2] == \"recommended\":\n",
    "                correct += 1\n",
    "    print(\"Recall: \", correct / total)\n",
    "    return correct / total\n",
    "\n",
    "def f1(precision, recall):\n",
    "    print(\"F1: \", (2 * precision * recall) / (precision + recall))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def use_AdditiveSmoothing_and_PreProcess(train_data, test_data):\n",
    "    new_train_data = pre_process(train_data)\n",
    "    new_test_data = pre_process(test_data)\n",
    "\n",
    "    final_test_data = probability_calculation(new_train_data, new_test_data, True)\n",
    "\n",
    "    print(\"Use Additive Smoothing and Pre Process Options: \")\n",
    "    wrong = accuracy(final_test_data)\n",
    "    _precision = precision(final_test_data)\n",
    "    _recall = recall(final_test_data)\n",
    "    f1(_precision, _recall)\n",
    "\n",
    "    for i in wrong:\n",
    "        print(i)\n",
    "\n",
    "def use_AdditiveSmoothing(train_data, test_data):\n",
    "    new_train_data = join_title_and_comment(train_data)\n",
    "    new_test_data = join_title_and_comment(test_data)\n",
    "\n",
    "    final_test_data = probability_calculation(new_train_data, new_test_data, True)\n",
    "\n",
    "    print(\"Use Additive Smoothing Option: \")\n",
    "    accuracy(final_test_data)\n",
    "    _precision = precision(final_test_data)\n",
    "    _recall = recall(final_test_data)\n",
    "    f1(_precision, _recall)\n",
    "\n",
    "def use_PreProcess(train_data, test_data):\n",
    "    new_train_data = pre_process(train_data)\n",
    "    new_test_data = pre_process(test_data)\n",
    "\n",
    "    final_test_data = probability_calculation(new_train_data, new_test_data, False)\n",
    "\n",
    "    print(\"Use Pre Process Option: \")\n",
    "    accuracy(final_test_data)\n",
    "    _precision = precision(final_test_data)\n",
    "    _recall = recall(final_test_data)\n",
    "    f1(_precision, _recall)\n",
    "\n",
    "def none_options(train_data, test_data):\n",
    "    new_train_data = join_title_and_comment(train_data)\n",
    "    new_test_data = join_title_and_comment(test_data)\n",
    "\n",
    "    final_test_data = probability_calculation(new_train_data, new_test_data, False)\n",
    "\n",
    "    print(\"None Options: \")\n",
    "    accuracy(final_test_data)\n",
    "    _precision = precision(final_test_data)\n",
    "    _recall = recall(final_test_data)\n",
    "    f1(_precision, _recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "direction": "rtl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None Options: \n",
      "Accuracy:  0.89625\n",
      "Precision:  0.9116883116883117\n",
      "Recall:  0.8775\n",
      "F1:  0.8942675159235669\n",
      "\n",
      "Use Pre Process Option: \n",
      "Accuracy:  0.90375\n",
      "Precision:  0.906801007556675\n",
      "Recall:  0.9\n",
      "F1:  0.903387703889586\n",
      "\n",
      "Use Additive Smoothing Option: \n",
      "Accuracy:  0.92625\n",
      "Precision:  0.8955916473317865\n",
      "Recall:  0.965\n",
      "F1:  0.9290012033694344\n",
      "\n",
      "Use Additive Smoothing and Pre Process Options: \n",
      "Accuracy:  0.9175\n",
      "Precision:  0.892018779342723\n",
      "Recall:  0.95\n",
      "F1:  0.9200968523002421\n",
      "\n",
      "2 رنگ میده یه وقتا موقع نوشتن قیم گزینه بهتر میشه . رو مینویسه رنگ میده یه وقتا موقع نوشتن  not_recommended recommended\n",
      "9 نقد خرید سلا راح شد#شو کابل شارژ توصیه میشود شد#شو . ارز گوش شارژ وایرلس مجهز .  recommended not_recommended\n",
      "13 mi ۴w کاور مقاو قشنگیه متأسفانه زیادیه ناموجوده . . . موجود بشه قطعا سفار می‌د . رنگ طلا فوق‌العاده زیباس طراح لبه نحویه دس اذ نمی‌کنه . لطفا موجود بشه ممنون  not_recommended recommended\n",
      "19 نقد منصفانه تخفیف ۵ خرید همشون تس میگن دوس خرابه نیومده انتقال فایل میشه کامپیو اون مشکل ندارن شارژ سریع تس اون اوک هس دوس کابل شار سریع باشه گوشیتون سریع شارژ بشه کلگ شارژ گوش قابل شارژ سریع باشه مشکل جنس س یک کیف هس امیدوار پاره نشن عمر کوتاهس ۱ ۳ ماه دوا نداره یک هفته دوا آورد#آور کابل طرح هستن فابریک سامسونگ نیستن  not_recommended recommended\n",
      "67 اندازه اندازه کوچیک ین دکمه فشار میکنه مجبور میش داخل دکمه خال کن بتون از استفاده کن . واسه اینطور  not_recommended recommended\n"
     ]
    }
   ],
   "source": [
    "def get_file_data(file_name):\n",
    "    table_info = list()\n",
    "\n",
    "    with open(file_name, mode = 'r') as file:     \n",
    "        csv_file = csv.reader(file)  \n",
    "        for line in csv_file:    \n",
    "            table_info.append(line) \n",
    "    return table_info\n",
    "\n",
    "def main():\n",
    "    train_data = get_file_data(\"comment_train.csv\")\n",
    "    test_data = get_file_data(\"comment_test.csv\")\n",
    "    none_options(train_data, test_data)\n",
    "\n",
    "    train_data = get_file_data(\"comment_train.csv\")\n",
    "    test_data = get_file_data(\"comment_test.csv\")\n",
    "    use_PreProcess(get_file_data(\"comment_train.csv\"), get_file_data(\"comment_test.csv\"))\n",
    "\n",
    "    train_data = get_file_data(\"comment_train.csv\")\n",
    "    test_data = get_file_data(\"comment_test.csv\")\n",
    "    use_AdditiveSmoothing(train_data, test_data)\n",
    "\n",
    "    train_data = get_file_data(\"comment_train.csv\")\n",
    "    test_data = get_file_data(\"comment_test.csv\")\n",
    "    use_AdditiveSmoothing_and_PreProcess(train_data, test_data)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Question 9:\n",
    "In this question, we have given 5 of the comments that have been evaluated incorrectly. In order, from left to right, the comment number, the text of the comment, whether the comment is recommended or not, and then our evaluation of the comment are given. One of the reasons is Misdiagnosis and evaluation can be irrelevant comments and titles. Another reason could be to remove positive and negative prefixes from verbs or adjectives. Another reason could be that the user uses positive words for another product in his opinion and the evaluation algorithm is wrong. Or even some opinions are difficult for humans to evaluate and therefore the algorithm can not correctly diagnose."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
